{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "name": "SIBDL-demo-group-13.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymohamedahmed/drbayes/blob/master/SIBDL_demo_group_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1HFtcUoANXK"
      },
      "source": [
        "# Machine Learning Seminars - Subspace Inference for Bayesian Deep Learning - Demo\n",
        "\n",
        "*Reviewed by Chiara Campagnola, Yousuf Mohamed-Ahmed and Hannah Teufel*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtjBG4O5AU0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124ea43d-06a9-4cc5-9656-d9492f4fb355"
      },
      "source": [
        "!rm -rf drbayes\n",
        "!git clone https://github.com/ymohamedahmed/drbayes.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'drbayes'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 334 (delta 32), reused 42 (delta 15), pack-reused 261\u001b[K\n",
            "Receiving objects: 100% (334/334), 11.60 MiB | 15.91 MiB/s, done.\n",
            "Resolving deltas: 100% (102/102), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03E9eUMjAvMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aedc3bf1-2622-4702-d9ce-66c7a652aadc"
      },
      "source": [
        "!pip install -e drbayes"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/drbayes\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from subspace-inference==0.0) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from subspace-inference==0.0) (1.19.5)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from subspace-inference==0.0) (0.8.2+cu101)\n",
            "Collecting gpytorch>=0.1.0rc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/d0/96634a8ae84b08bd64709c1abd4f319a70f404967c598690bca8be143fb8/gpytorch-1.4.0.tar.gz (286kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from subspace-inference==0.0) (0.8.9)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from subspace-inference==0.0) (1.4.1)\n",
            "Requirement already satisfied: setuptools>=39.1.0 in /usr/local/lib/python3.7/dist-packages (from subspace-inference==0.0) (54.0.0)\n",
            "Requirement already satisfied: matplotlib>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from subspace-inference==0.0) (3.2.2)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from subspace-inference==0.0) (1.7.1+cu101)\n",
            "Requirement already satisfied: scikit_learn>=0.20.2 in /usr/local/lib/python3.7/dist-packages (from subspace-inference==0.0) (0.22.2.post1)\n",
            "Collecting pyro-ppl==1.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4d/e45ff02364438ce8698ed70b1fbd9240f7c4f6e509fb90e9c04657f895b5/pyro_ppl-1.5.2-py3-none-any.whl (607kB)\n",
            "\u001b[K     |████████████████████████████████| 614kB 18.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.4.5 in /usr/local/lib/python3.7/dist-packages (from subspace-inference==0.0) (2.4.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from subspace-inference==0.0) (7.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->subspace-inference==0.0) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->subspace-inference==0.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->subspace-inference==0.0) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->subspace-inference==0.0) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>=0.20.2->subspace-inference==0.0) (1.0.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl==1.5.2->subspace-inference==0.0) (3.3.0)\n",
            "Collecting pyro-api>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/81/957ae78e6398460a7230b0eb9b8f1cb954c5e913e868e48d89324c68cec7/pyro_api-0.1.2-py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2.2->subspace-inference==0.0) (1.15.0)\n",
            "Building wheels for collected packages: gpytorch\n",
            "  Building wheel for gpytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpytorch: filename=gpytorch-1.4.0-py2.py3-none-any.whl size=477826 sha256=d542a01a2baf09e79d45c2316820a2e74a5bd43d348ee13b52f735f893b2d277\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/f5/39/404e1875f841e8a999e94a7efa17f6ef900298be5452b63b0c\n",
            "Successfully built gpytorch\n",
            "Installing collected packages: gpytorch, pyro-api, pyro-ppl, subspace-inference\n",
            "  Running setup.py develop for subspace-inference\n",
            "Successfully installed gpytorch-1.4.0 pyro-api-0.1.2 pyro-ppl-1.5.2 subspace-inference\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTnpHtKDANXP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f4083fc-a699-47d6-baf0-5bc9942ff5b9"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import subspace_inference\n",
        "import subspace_inference.utils as utils\n",
        "from subspace_inference.posteriors import SWAG\n",
        "from subspace_inference import models, losses, utils\n",
        "from subspace_inference.models import MLP\n",
        "from subspace_inference.visualization import plot_predictive\n",
        "from subspace_inference.posteriors.proj_model import SubspaceModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.manual_seed(1)\n",
        "torch.cuda.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UwutCr-CoVz"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqOHeNvdANXS"
      },
      "source": [
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "train = datasets.QMNIST(root=\"../data\",train=True, download=True,\n",
        "                   transform=transform)\n",
        "test = datasets.QMNIST('../data', train=False,download=True,\n",
        "                   transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train,batch_size=512)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gchOCSV-ANXT"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcwPHp__H-cs"
      },
      "source": [
        "def train(model, loader, optimizer, criterion, lr_init=1e-2, epochs=3000, \n",
        "          swag_model=None, swag=False, swag_start=2000, swag_freq=50, swag_lr=1e-3,\n",
        "          print_freq=100):\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        t = (epoch + 1) / swag_start if swag else (epoch + 1) / epochs\n",
        "        lr_ratio = swag_lr / lr_init if swag else 0.05\n",
        "        \n",
        "        if t <= 0.5:\n",
        "            factor = 1.0\n",
        "        elif t <= 0.9:\n",
        "            factor = 1.0 - (1.0 - lr_ratio) * (t - 0.5) / 0.4\n",
        "        else:\n",
        "            factor = lr_ratio\n",
        "\n",
        "        lr = factor * lr_init\n",
        "        utils.adjust_learning_rate(optimizer, lr)\n",
        "        \n",
        "        train_res = utils.train_epoch(loader, model, criterion, optimizer, cuda=False, regression=False)\n",
        "        if swag and epoch > swag_start:\n",
        "            swag_model.collect_model(model)\n",
        "        \n",
        "        if (epoch % print_freq == 0 or epoch == epochs - 1):\n",
        "            print('Epoch %d. LR: %g. Loss: %.4f' % (epoch, lr, train_res['loss']))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "tzzvWuu7ItK5",
        "outputId": "035bf15e-6f48-430f-85f7-d3a67938a2fc"
      },
      "source": [
        "wd = 0.\n",
        "lr_init = 1e-2\n",
        "\n",
        "model_cfg = models.ToyRegNet\n",
        "criterion = losses.GaussianLikelihood(noise_var=1.)\n",
        "criterion = F.cross_entropy\n",
        "model_cfg.kwargs = {\"dimensions\":[20,20], \"output_dim\":10, \"input_dim\":28*28}\n",
        "model = model_cfg.base(*model_cfg.args, **model_cfg.kwargs)\n",
        "for i in range(2):\n",
        "    print(\"Training Model\", i)\n",
        "    swag_model = SWAG(model_cfg.base, subspace_type=\"pca\", *model_cfg.args, **model_cfg.kwargs, \n",
        "                  subspace_kwargs={\"max_rank\": 10, \"pca_rank\": 10})\n",
        "    model = model_cfg.base(*model_cfg.args, **model_cfg.kwargs)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr_init, momentum=0.95, weight_decay=wd)\n",
        "    \n",
        "    train(model, train_loader, optimizer, criterion, lr_init, 3000, print_freq=1000, \n",
        "          swag=True, swag_model=swag_model, swag_start=2000, swag_freq=10, swag_lr=1e-2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Model 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleAttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-651a57c06299>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     train(model, train_loader, optimizer, criterion, lr_init, 3000, print_freq=1000, \n\u001b[0;32m---> 17\u001b[0;31m           swag=True, swag_model=swag_model, swag_start=2000, swag_freq=10, swag_lr=1e-2)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-0ee200a47211>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, lr_init, epochs, swag_model, swag, swag_start, swag_freq, swag_lr, print_freq)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswag\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mswag_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mswag_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drbayes/subspace_inference/utils.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(loader, model, criterion, optimizer, cuda, regression, verbose, subset)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 779\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleAttributeError\u001b[0m: 'RegNetBase' object has no attribute 'log_softmax'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THx5CUJMDa5u"
      },
      "source": [
        "\n",
        "def train(model, loss_function, max_epochs, train_loader):\n",
        "  optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "  swag_model = SWAG(VanillaMLP, subspace_type=\"pca\",\n",
        "                  subspace_kwargs={\"max_rank\": 10, \"pca_rank\": 10},dims=[28*28,50,20,10])\n",
        "  for epoch in range(max_epochs):\n",
        "    total_loss = 0\n",
        "    for x,y in train_loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      # out = F.softmax(model(x.flatten()))\n",
        "      out = model(x)\n",
        "      # swag_model.collect_model(model)\n",
        "      loss = loss_function(out,y)\n",
        "      total_loss += loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch: {epoch}, loss: {loss}\")\n",
        "\n",
        "  return swag_model.get_space()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raWEYW_IANXT"
      },
      "source": [
        "### Mean Field Variational Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyFkYb7MANXU"
      },
      "source": [
        "### Ensembles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acED4SKvANXU"
      },
      "source": [
        "### Neural network + Bayesian Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw3J4TWtANXV"
      },
      "source": [
        "### _SIBDL_: PCA subspace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhcN0Zwa9Khx"
      },
      "source": [
        "class VanillaMLP(nn.Module):\n",
        "  def __init__(self, dims):\n",
        "    super(VanillaMLP,self).__init__()\n",
        "    layers = [nn.Flatten()] + [lay for (x,y) in zip(dims[:-1],dims[1:]) for lay in [nn.Linear(x,y), nn.ReLU()] ]\n",
        "    layers.pop()\n",
        "    \n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.model(x)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otw8B26Uz4Lt"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7PAwTtoANXW"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsvlp-ia8cZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d718a913-14e7-4119-d67d-a91cc384d3ff"
      },
      "source": [
        "model = VanillaMLP([28*28,50,20,10])\n",
        "print(model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VanillaMLP(\n",
            "  (model): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=784, out_features=50, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=50, out_features=20, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=20, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ7DJHwhANXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40cddc10-240e-44d4-87f8-37ec10bc268e"
      },
      "source": [
        "model = VanillaMLP([28*28,50,20,10])\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "space = train(model, F.cross_entropy, 20, train_loader)\n",
        "torch.save(model.state_dict(), \"MLP.pt\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, loss: 0.3758305609226227\n",
            "Epoch: 10, loss: 0.06851270794868469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAtok3HXsbOf",
        "outputId": "a0ed1346-11c3-446e-dc57-5246ff1155e5"
      },
      "source": [
        "space"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-0.0278, -0.0378, -0.0119,  ...,  0.0468, -0.1616, -0.1590]),\n",
              " tensor([2.8196e-05, 2.8204e-05, 2.8196e-05,  ..., 5.7454e-05, 1.1363e-04,\n",
              "         1.0563e-04]),\n",
              " tensor([[-1.6371e-03, -1.6370e-03, -1.6369e-03,  ..., -7.9936e-03,\n",
              "           1.1685e-02, -1.0416e-02],\n",
              "         [ 5.1491e-04,  5.1496e-04,  5.1497e-04,  ...,  3.8997e-04,\n",
              "          -3.6317e-04, -1.1911e-04],\n",
              "         [ 7.3030e-05,  7.2600e-05,  7.2626e-05,  ..., -1.7019e-04,\n",
              "          -2.0231e-04,  3.5258e-05],\n",
              "         ...,\n",
              "         [ 1.1435e-05,  1.1249e-05,  1.1269e-05,  ..., -4.4610e-07,\n",
              "          -1.8295e-06,  4.5624e-06],\n",
              "         [-2.6892e-07, -1.1821e-06, -1.1737e-06,  ..., -1.9457e-07,\n",
              "           3.0103e-06,  6.0429e-07],\n",
              "         [ 3.8554e-07, -7.5043e-07, -7.3901e-07,  ...,  1.6949e-06,\n",
              "           1.4114e-06,  8.9324e-07]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "ZjyYWrfldOkJ",
        "outputId": "7e63c17c-f3f0-4832-a202-1455f2865544"
      },
      "source": [
        "  model_cfg.args"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2e2c792c7fe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_cfg' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZCPBVTTANXY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "e3ef8b3c-cbb4-4d8e-a877-ae09be044644"
      },
      "source": [
        "from subspace_inference.posteriors.vi_model import VIModel, ELBO\n",
        "import math\n",
        "def get_pca_space():\n",
        "    # swag_model = SWAG(model_cfg.base, subspace_type=\"pca\", *model_cfg.args, **model_cfg.kwargs, \n",
        "    #               subspace_kwargs={\"max_rank\": 10, \"pca_rank\": 10})\n",
        "    # print(torch.load(\"MLP.pt\").keys())\n",
        "    # swag_model.load_state_dict(torch.load(\"MLP.pt\"))#[\"state_dict\"])\n",
        "    # mean, _, cov_factor = swag_model.get_space()\n",
        "    mean, _, cov_factor = space\n",
        "    subspace = SubspaceModel(mean, cov_factor)\n",
        "    return subspace\n",
        "\n",
        "subspace = get_pca_space()\n",
        "init_sigma = 1.\n",
        "prior_sigma = 5.\n",
        "criterion = losses.GaussianLikelihood(noise_var=.05)\n",
        "temperature = 1.\n",
        "\n",
        "vi_model = VIModel(\n",
        "    subspace=subspace,\n",
        "    init_inv_softplus_sigma=math.log(math.exp(init_sigma) - 1.0),\n",
        "    prior_log_sigma=math.log(prior_sigma),\n",
        "    base=VanillaMLP,\n",
        "    dims=[28*28,50,20,10]\n",
        ")\n",
        "\n",
        "elbo = ELBO(criterion, len(train_loader.dataset), temperature=temperature)\n",
        "optimizer = torch.optim.Adam([param for param in vi_model.parameters()], lr=.1)\n",
        "vi_model.to(device)\n",
        "train(vi_model, criterion, 30, train_loader)\n",
        "# for epoch in range(2000):\n",
        "#     optimizer.zero_grad()\n",
        "#     train_res = utils.train_epoch(loader, vi_model, elbo, optimizer, regression=True, cuda=False)\n",
        "#     sigma = torch.nn.functional.softplus(vi_model.inv_softplus_sigma.detach().cpu())\n",
        "#     if epoch % 1000 == 0 or epoch == 1999:\n",
        "#         print(epoch, train_res)\n",
        "#     if epoch == 1000:\n",
        "#         utils.adjust_learning_rate(optimizer, 0.01)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-76179e880adf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvi_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mvi_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvi_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m# for epoch in range(2000):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#     optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-d231c8934fa0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_function, max_epochs, train_loader)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   swag_model = SWAG(VanillaMLP, subspace_type=\"pca\",\n\u001b[0;32m----> 5\u001b[0;31m                   subspace_kwargs={\"max_rank\": 10, \"pca_rank\": 10},dims=[28*28,50,20,10])\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drbayes/subspace_inference/posteriors/swag.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base, subspace_type, subspace_kwargs, var_clamp, *args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     def __init__(self, base, subspace_type,\n\u001b[1;32m     10\u001b[0m                  subspace_kwargs=None, var_clamp=1e-6, *args, **kwargs):\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSWAG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4TxmtQbANXY"
      },
      "source": [
        "## _References_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ8oHrcOANXY"
      },
      "source": [
        "- A very useful repository for a lot of Bayesian NN implementations: https://github.com/JavierAntoran/Bayesian-Neural-Networks\n",
        "- The code for the paper is found at https://github.com/wjmaddox/drbayes and specifically the following notebook was adapted for this demonstration (https://github.com/wjmaddox/drbayes/blob/master/experiments/synthetic_regression/visualizing_uncertainty.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxYdtvcTANXZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}